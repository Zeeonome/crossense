<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>crossense.ensemble API documentation</title>
<meta name="description" content="The :mod:`crossense.ensemble` module includes ensemble-based methods for
classification, regression and anomaly detection." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>crossense.ensemble</code></h1>
</header>
<section id="section-intro">
<p>The :mod:<code><a title="crossense.ensemble" href="#crossense.ensemble">crossense.ensemble</a></code> module includes ensemble-based methods for
classification, regression and anomaly detection.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
The :mod:`crossense.ensemble` module includes ensemble-based methods for
classification, regression and anomaly detection.
&#34;&#34;&#34;
from ._bagging import BaseCrossBagging, CrossBaggingClassifier, CrossBaggingRegressor

__all__ = [
    &#34;BaseCrossBagging&#34;,
    &#34;CrossBaggingClassifier&#34;,
    &#34;CrossBaggingRegressor&#34;,
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="crossense.ensemble.tests" href="tests/index.html">crossense.ensemble.tests</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="crossense.ensemble.BaseCrossBagging"><code class="flex name class">
<span>class <span class="ident">BaseCrossBagging</span></span>
<span>(</span><span>estimator=None, cv=5, *, n_jobs=None, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for cross-fold Bagging meta-estimator.</p>
<p>Warning: This class should not be used directly. Use derived classes
instead.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseCrossBagging(BaseEnsemble, metaclass=ABCMeta):
    &#34;&#34;&#34;Base class for cross-fold Bagging meta-estimator.

    Warning: This class should not be used directly. Use derived classes
    instead.
    &#34;&#34;&#34;

    _parameter_constraints: dict = {
        &#34;estimator&#34;: [HasMethods([&#34;fit&#34;, &#34;predict&#34;]), None],
        &#34;n_jobs&#34;: [None, Integral],
        &#34;random_state&#34;: [&#34;random_state&#34;],
        &#34;verbose&#34;: [&#34;verbose&#34;],
    }

    @abstractmethod
    def __init__(
        self,
        estimator=None,
        cv=5,
        *,
        n_jobs=None,
        verbose=0,
    ):
        self.cv: _BaseKFold = check_cv(cv, classifier=is_classifier(estimator))
        super().__init__(
            estimator=estimator,
            n_estimators=self.cv.n_splits,
        )
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.estimators_ = []
        self.estimators_samples_ = []

    @_fit_context(
        # BaseBagging.estimator is not validated yet
        prefer_skip_nested_validation=False
    )
    def fit(self, X, y, sample_weight=None):
        &#34;&#34;&#34;Build a Bagging ensemble of estimators from the training set (X, y).

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        y : array-like of shape (n_samples,)
            The target values (class labels in classification, real numbers in
            regression).

        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights. If None, then samples are equally weighted.
            Note that this is supported only if the base estimator supports
            sample weighting.
        &#34;&#34;&#34;
        # Convert data (X is required to be 2d and indexable)
        X, y = self._validate_data(
            X,
            y,
            accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
            dtype=None,
            force_all_finite=False,
            multi_output=True,
        )
        return self._fit(X, y, sample_weight=sample_weight)

    # noinspection PyMethodMayBeStatic
    def _parallel_args(self):
        return {}

    def _fit(
        self,
        X,
        y,
        max_depth=None,
        sample_weight=None,
        check_input=True,
    ):
        &#34;&#34;&#34;Build a Bagging ensemble of estimators from the training
           set (X, y).

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        y : array-like of shape (n_samples,)
            The target values (class labels in classification, real numbers in
            regression).

        max_depth : int, default=None
            Override value used when constructing base estimator. Only
            supported if the base estimator has a max_depth parameter.

        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights. If None, then samples are equally weighted.
            Note that this is supported only if the base estimator supports
            sample weighting.

        check_input : bool, default=True
            Override value used when fitting base estimator. Only supported
            if the base estimator has a check_input parameter for fit function.
        &#34;&#34;&#34;
        self._generate_fold_indices(X, y, None)
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X, dtype=None)

        # Remap output
        n_samples = X.shape[0]
        self._n_samples = n_samples
        y = self._validate_y(y)

        # Check parameters
        self._validate_estimator()

        if max_depth is not None:
            self.estimator_.max_depth = max_depth

        # Parallel loop
        n_jobs, n_estimators, starts = _partition_estimators(
            self.n_estimators, self.n_jobs
        )
        total_n_estimators = sum(n_estimators)

        all_results = Parallel(
            n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args()
        )(
            delayed(_parallel_build_estimators)(
                n_estimators[i],
                self,
                self.estimators_samples_[starts[i] : starts[i + 1]],
                X,
                y,
                sample_weight,
                total_n_estimators,
                verbose=self.verbose,
                check_input=check_input,
            )
            for i in range(n_jobs)
        )

        # Reduce
        self.estimators_ = list(
            itertools.chain.from_iterable(t[0] for t in all_results)
        )
        return self

    # noinspection PyMethodMayBeStatic
    def _validate_y(self, y):
        if len(y.shape) == 1 or y.shape[1] == 1:
            return column_or_1d(y, warn=True)
        return y

    def _generate_fold_indices(self, X, y, groups):
        check_is_fitted(self)
        for fold in self.cv.split(X, y, groups):
            self.estimators_samples_.append(fold[0])

    def set_params(self, **params):
        cv = params.pop(&#34;cv&#34;, None)
        if cv:
            self.cv = check_cv(cv, classifier=is_classifier(self.estimator))
        return super().set_params(**params)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.ensemble._base.BaseEnsemble</li>
<li>sklearn.base.MetaEstimatorMixin</li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>crossense.ensemble._bagging.CrossBaggingClassifier</li>
<li>crossense.ensemble._bagging.CrossBaggingRegressor</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="crossense.ensemble.BaseCrossBagging.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, sample_weight=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Build a Bagging ensemble of estimators from the training set (X, y).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code></dt>
<dd>The target values (class labels in classification, real numbers in
regression).</dd>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code>, default=<code>None</code></dt>
<dd>Sample weights. If None, then samples are equally weighted.
Note that this is supported only if the base estimator supports
sample weighting.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_fit_context(
    # BaseBagging.estimator is not validated yet
    prefer_skip_nested_validation=False
)
def fit(self, X, y, sample_weight=None):
    &#34;&#34;&#34;Build a Bagging ensemble of estimators from the training set (X, y).

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    y : array-like of shape (n_samples,)
        The target values (class labels in classification, real numbers in
        regression).

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights. If None, then samples are equally weighted.
        Note that this is supported only if the base estimator supports
        sample weighting.
    &#34;&#34;&#34;
    # Convert data (X is required to be 2d and indexable)
    X, y = self._validate_data(
        X,
        y,
        accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
        dtype=None,
        force_all_finite=False,
        multi_output=True,
    )
    return self._fit(X, y, sample_weight=sample_weight)</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.BaseCrossBagging.set_fit_request"><code class="name flex">
<span>def <span class="ident">set_fit_request</span></span>(<span>self: crossense.ensemble._bagging.BaseCrossBagging, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> crossense.ensemble._bagging.BaseCrossBagging</span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>fit</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>fit</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.BaseCrossBagging.set_params"><code class="name flex">
<span>def <span class="ident">set_params</span></span>(<span>self, **params)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as :class:<code>~sklearn.pipeline.Pipeline</code>). The latter have
parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's
possible to update each component of a nested object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Estimator parameters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>estimator instance</code></dt>
<dd>Estimator instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_params(self, **params):
    cv = params.pop(&#34;cv&#34;, None)
    if cv:
        self.cv = check_cv(cv, classifier=is_classifier(self.estimator))
    return super().set_params(**params)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="crossense.ensemble.CrossBaggingClassifier"><code class="flex name class">
<span>class <span class="ident">CrossBaggingClassifier</span></span>
<span>(</span><span>estimator: object = None, cv: Union[int, BaseCrossValidator, Iterable] = 5, *, n_jobs: Optional[int] = None, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>A cross-validation Bagging classifier.</p>
<p>A Bagging classifier is an ensemble meta-estimator that fits base
classifiers each on a fold of cross-validation generator</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>estimator_</code></strong> :&ensp;<code>estimator</code></dt>
<dd>The base estimator from which the ensemble is grown.</dd>
<dt><strong><code>n_features_in_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features seen during :term:<code>fit</code>.</dd>
<dt><strong><code>feature_names_in_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (</code>n_features_in_<code>,)</code></dt>
<dd>Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
has feature names that are all strings.</dd>
<dt><strong><code>estimators_</code></strong> :&ensp;<code>list</code> of <code>estimators</code></dt>
<dd>The collection of fitted base estimators.</dd>
<dt><strong><code>estimators_samples_</code></strong> :&ensp;<code>list</code> of <code>arrays</code></dt>
<dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</dd>
<dt><strong><code>classes_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_classes,)</code></dt>
<dd>The classes labels.</dd>
<dt><strong><code>n_classes_</code></strong> :&ensp;<code>int</code> or <code>list</code></dt>
<dd>The number of classes.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; from crossense.ensemble import CrossBaggingClassifier
&gt;&gt;&gt; from sklearn.datasets import make_classification
&gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
&gt;&gt;&gt; clf = CrossBaggingClassifier(estimator=SVC(), cv=5).fit(X, y)
&gt;&gt;&gt; clf.predict([[0, 0, 0, 0]])
array([1])
</code></pre>
<h2 id="parameters">Parameters</h2>
<p>estimator:
The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a
:class:<code>~sklearn.tree.DecisionTreeClassifier</code>.</p>
<p>cv:
Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<pre><code>- &lt;code&gt;None&lt;/code&gt;, to use the default 5-fold cross validation,
- int, to specify the number of folds in a &lt;code&gt;(Stratified)KFold&lt;/code&gt;,
- :term:&lt;code&gt;CV splitter&lt;/code&gt;,
- An iterable that generates (train, test) splits as arrays of indices.

For &lt;code&gt;int&lt;/code&gt;/&lt;code&gt;None&lt;/code&gt; inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is
either binary or multiclass, :class:&lt;code&gt;StratifiedKFold&lt;/code&gt; is used. In all
other cases, :class:&lt;code&gt;KFold&lt;/code&gt; is used. These splitters are instantiated
with `shuffle=False` so the splits will be the same across calls.

Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
cross-validation strategies that can be used here.
</code></pre>
<p>n_jobs:
The number of jobs to run in parallel for both :meth:<code>fit</code> and
:meth:<code>predict</code>. <code>None</code> means 1 unless in a
:obj:<code>joblib.parallel_backend</code> context. <code>-1</code> means using all
processors. See :term:<code>Glossary &lt;n_jobs&gt;</code> for more details.</p>
<p>verbose:
Controls the verbosity when fitting and predicting.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CrossBaggingClassifier(ClassifierMixin, BaseCrossBagging):
    &#34;&#34;&#34;A cross-validation Bagging classifier.

    A Bagging classifier is an ensemble meta-estimator that fits base
    classifiers each on a fold of cross-validation generator

    Attributes
    ----------
    estimator_ : estimator
        The base estimator from which the ensemble is grown.

    n_features_in_ : int
        Number of features seen during :term:`fit`.

    feature_names_in_ : ndarray of shape (`n_features_in_`,)
        Names of features seen during :term:`fit`. Defined only when `X`
        has feature names that are all strings.

    estimators_ : list of estimators
        The collection of fitted base estimators.

    estimators_samples_ : list of arrays
        The subset of drawn samples (i.e., the in-bag samples) for each base
        estimator. Each subset is defined by an array of the indices selected.

    classes_ : ndarray of shape (n_classes,)
        The classes labels.

    n_classes_ : int or list
        The number of classes.

    Examples
    --------
    &gt;&gt;&gt; from sklearn.svm import SVC
    &gt;&gt;&gt; from crossense.ensemble import CrossBaggingClassifier
    &gt;&gt;&gt; from sklearn.datasets import make_classification
    &gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=4,
    ...                            n_informative=2, n_redundant=0,
    ...                            random_state=0, shuffle=False)
    &gt;&gt;&gt; clf = CrossBaggingClassifier(estimator=SVC(), cv=5).fit(X, y)
    &gt;&gt;&gt; clf.predict([[0, 0, 0, 0]])
    array([1])
    &#34;&#34;&#34;

    def __init__(
        self,
        estimator: object = None,
        cv: Union[int, BaseCrossValidator, Iterable] = 5,
        *,
        n_jobs: Optional[int] = None,
        verbose=0,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        estimator:
            The base estimator to fit on random subsets of the dataset.
            If None, then the base estimator is a
            :class:`~sklearn.tree.DecisionTreeClassifier`.

        cv:
            Determines the cross-validation splitting strategy.
            Possible inputs for cv are:

            - `None`, to use the default 5-fold cross validation,
            - int, to specify the number of folds in a `(Stratified)KFold`,
            - :term:`CV splitter`,
            - An iterable that generates (train, test) splits as arrays of indices.

            For `int`/`None` inputs, if the estimator is a classifier and `y` is
            either binary or multiclass, :class:`StratifiedKFold` is used. In all
            other cases, :class:`KFold` is used. These splitters are instantiated
            with `shuffle=False` so the splits will be the same across calls.

            Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
            cross-validation strategies that can be used here.

        n_jobs:
            The number of jobs to run in parallel for both :meth:`fit` and
            :meth:`predict`. ``None`` means 1 unless in a
            :obj:`joblib.parallel_backend` context. ``-1`` means using all
            processors. See :term:`Glossary &lt;n_jobs&gt;` for more details.

        verbose:
            Controls the verbosity when fitting and predicting.
        &#34;&#34;&#34;
        super().__init__(
            estimator=estimator,
            cv=cv,
            n_jobs=n_jobs,
            verbose=verbose,
        )

    def _validate_estimator(self, default=None):
        &#34;&#34;&#34;Check the estimator and set the estimator_ attribute.&#34;&#34;&#34;
        super()._validate_estimator(default=DecisionTreeClassifier())

    def _validate_y(self, y):
        y = column_or_1d(y, warn=True)
        check_classification_targets(y)
        self.classes_, y = np.unique(y, return_inverse=True)
        self.n_classes_ = len(self.classes_)

        return y

    def predict_all_proba(self, X):
        &#34;&#34;&#34;Predict class probabilities of all models for X.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        p : ndarray of shape (n_estimators, n_samples, n_classes)
            The class probabilities of the input samples. The order of the
            classes corresponds to that in the attribute :term:`classes_`.
        &#34;&#34;&#34;
        check_is_fitted(self)
        # Check data
        X = self._validate_data(
            X,
            accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
            dtype=None,
            force_all_finite=False,
            reset=False,
        )

        # Parallel loop
        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

        all_proba = Parallel(
            n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args()
        )(
            delayed(_parallel_predict_proba)(
                self.estimators_[starts[i] : starts[i + 1]],
                X,
                self.n_classes_,
            )
            for i in range(n_jobs)
        )
        all_proba = list(itertools.chain.from_iterable(all_proba))
        return np.concatenate([x[np.newaxis, :, :] for x in all_proba], axis=0)

    def predict(self, X):
        &#34;&#34;&#34;Predict class for X.

        The predicted class of an input sample is computed as the class with
        the highest mean predicted probability. If base estimators do not
        implement a ``predict_proba`` method, then it resorts to voting.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        y : ndarray of shape (n_samples,)
            The predicted classes.
        &#34;&#34;&#34;
        predicted_probabilitiy = self.predict_proba(X)
        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)), axis=0)

    def predict_proba(self, X):
        &#34;&#34;&#34;Predict class probabilities for X.

        The predicted class probabilities of an input sample is computed as
        the mean predicted class probabilities of the base estimators in the
        ensemble. If base estimators do not implement a ``predict_proba``
        method, then it resorts to voting and the predicted class probabilities
        of an input sample represents the proportion of estimators predicting
        each class.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        p : ndarray of shape (n_samples, n_classes)
            The class probabilities of the input samples. The order of the
            classes corresponds to that in the attribute :term:`classes_`.
        &#34;&#34;&#34;
        all_proba = self.predict_all_proba(X)
        # Reduce
        proba = all_proba.mean(axis=0)

        return proba

    def predict_log_proba(self, X):
        &#34;&#34;&#34;Predict class log-probabilities for X.

        The predicted class log-probabilities of an input sample is computed as
        the log of the mean predicted class probabilities of the base
        estimators in the ensemble.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        p : ndarray of shape (n_samples, n_classes)
            The class log-probabilities of the input samples. The order of the
            classes corresponds to that in the attribute :term:`classes_`.
        &#34;&#34;&#34;
        check_is_fitted(self)
        if hasattr(self.estimator_, &#34;predict_log_proba&#34;):
            # Check data
            X = self._validate_data(
                X,
                accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
                dtype=None,
                force_all_finite=False,
                reset=False,
            )

            # Parallel loop
            n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
                delayed(_parallel_predict_log_proba)(
                    self.estimators_[starts[i] : starts[i + 1]],
                    X,
                    self.n_classes_,
                )
                for i in range(n_jobs)
            )

            # Reduce
            log_proba = all_log_proba[0]

            for j in range(1, len(all_log_proba)):
                log_proba = np.logaddexp(log_proba, all_log_proba[j])

            log_proba -= np.log(self.n_estimators)

        else:
            log_proba = np.log(self.predict_proba(X))

        return log_proba

    @available_if(_estimator_has(&#34;decision_function&#34;))
    def decision_function(self, X):
        &#34;&#34;&#34;Average of the decision functions of the base classifiers.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        score : ndarray of shape (n_samples, k)
            The decision function of the input samples. The columns correspond
            to the classes in sorted order, as they appear in the attribute
            ``classes_``. Regression and binary classification are special
            cases with ``k == 1``, otherwise ``k==n_classes``.
        &#34;&#34;&#34;
        # noinspection DuplicatedCode
        check_is_fitted(self)

        # Check data
        X = self._validate_data(
            X,
            accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
            dtype=None,
            force_all_finite=False,
            reset=False,
        )

        # Parallel loop
        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
            delayed(_parallel_decision_function)(
                self.estimators_[starts[i] : starts[i + 1]],
                X,
            )
            for i in range(n_jobs)
        )

        # Reduce
        decisions = sum(all_decisions) / self.n_estimators

        return decisions

    def _more_tags(self):
        if self.estimator is None:
            estimator = DecisionTreeClassifier()
        else:
            estimator = self.estimator

        return {&#34;allow_nan&#34;: _safe_tags(estimator, &#34;allow_nan&#34;)}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.ClassifierMixin</li>
<li>crossense.ensemble._bagging.BaseCrossBagging</li>
<li>sklearn.ensemble._base.BaseEnsemble</li>
<li>sklearn.base.MetaEstimatorMixin</li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="crossense.ensemble.CrossBaggingClassifier.decision_function"><code class="name flex">
<span>def <span class="ident">decision_function</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Average of the decision functions of the base classifiers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, k)</code></dt>
<dd>The decision function of the input samples. The columns correspond
to the classes in sorted order, as they appear in the attribute
<code>classes_</code>. Regression and binary classification are special
cases with <code>k == 1</code>, otherwise <code>k==n_classes</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@available_if(_estimator_has(&#34;decision_function&#34;))
def decision_function(self, X):
    &#34;&#34;&#34;Average of the decision functions of the base classifiers.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    Returns
    -------
    score : ndarray of shape (n_samples, k)
        The decision function of the input samples. The columns correspond
        to the classes in sorted order, as they appear in the attribute
        ``classes_``. Regression and binary classification are special
        cases with ``k == 1``, otherwise ``k==n_classes``.
    &#34;&#34;&#34;
    # noinspection DuplicatedCode
    check_is_fitted(self)

    # Check data
    X = self._validate_data(
        X,
        accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
        dtype=None,
        force_all_finite=False,
        reset=False,
    )

    # Parallel loop
    n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

    all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
        delayed(_parallel_decision_function)(
            self.estimators_[starts[i] : starts[i + 1]],
            X,
        )
        for i in range(n_jobs)
    )

    # Reduce
    decisions = sum(all_decisions) / self.n_estimators

    return decisions</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingClassifier.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict class for X.</p>
<p>The predicted class of an input sample is computed as the class with
the highest mean predicted probability. If base estimators do not
implement a <code>predict_proba</code> method, then it resorts to voting.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples,)</code></dt>
<dd>The predicted classes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;Predict class for X.

    The predicted class of an input sample is computed as the class with
    the highest mean predicted probability. If base estimators do not
    implement a ``predict_proba`` method, then it resorts to voting.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    Returns
    -------
    y : ndarray of shape (n_samples,)
        The predicted classes.
    &#34;&#34;&#34;
    predicted_probabilitiy = self.predict_proba(X)
    return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)), axis=0)</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingClassifier.predict_all_proba"><code class="name flex">
<span>def <span class="ident">predict_all_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict class probabilities of all models for X.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_estimators, n_samples, n_classes)</code></dt>
<dd>The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute :term:<code>classes_</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_all_proba(self, X):
    &#34;&#34;&#34;Predict class probabilities of all models for X.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    Returns
    -------
    p : ndarray of shape (n_estimators, n_samples, n_classes)
        The class probabilities of the input samples. The order of the
        classes corresponds to that in the attribute :term:`classes_`.
    &#34;&#34;&#34;
    check_is_fitted(self)
    # Check data
    X = self._validate_data(
        X,
        accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
        dtype=None,
        force_all_finite=False,
        reset=False,
    )

    # Parallel loop
    n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

    all_proba = Parallel(
        n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args()
    )(
        delayed(_parallel_predict_proba)(
            self.estimators_[starts[i] : starts[i + 1]],
            X,
            self.n_classes_,
        )
        for i in range(n_jobs)
    )
    all_proba = list(itertools.chain.from_iterable(all_proba))
    return np.concatenate([x[np.newaxis, :, :] for x in all_proba], axis=0)</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingClassifier.predict_log_proba"><code class="name flex">
<span>def <span class="ident">predict_log_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict class log-probabilities for X.</p>
<p>The predicted class log-probabilities of an input sample is computed as
the log of the mean predicted class probabilities of the base
estimators in the ensemble.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_classes)</code></dt>
<dd>The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute :term:<code>classes_</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_log_proba(self, X):
    &#34;&#34;&#34;Predict class log-probabilities for X.

    The predicted class log-probabilities of an input sample is computed as
    the log of the mean predicted class probabilities of the base
    estimators in the ensemble.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    Returns
    -------
    p : ndarray of shape (n_samples, n_classes)
        The class log-probabilities of the input samples. The order of the
        classes corresponds to that in the attribute :term:`classes_`.
    &#34;&#34;&#34;
    check_is_fitted(self)
    if hasattr(self.estimator_, &#34;predict_log_proba&#34;):
        # Check data
        X = self._validate_data(
            X,
            accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
            dtype=None,
            force_all_finite=False,
            reset=False,
        )

        # Parallel loop
        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

        all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
            delayed(_parallel_predict_log_proba)(
                self.estimators_[starts[i] : starts[i + 1]],
                X,
                self.n_classes_,
            )
            for i in range(n_jobs)
        )

        # Reduce
        log_proba = all_log_proba[0]

        for j in range(1, len(all_log_proba)):
            log_proba = np.logaddexp(log_proba, all_log_proba[j])

        log_proba -= np.log(self.n_estimators)

    else:
        log_proba = np.log(self.predict_proba(X))

    return log_proba</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingClassifier.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the mean predicted class probabilities of the base estimators in the
ensemble. If base estimators do not implement a <code>predict_proba</code>
method, then it resorts to voting and the predicted class probabilities
of an input sample represents the proportion of estimators predicting
each class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_classes)</code></dt>
<dd>The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute :term:<code>classes_</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X):
    &#34;&#34;&#34;Predict class probabilities for X.

    The predicted class probabilities of an input sample is computed as
    the mean predicted class probabilities of the base estimators in the
    ensemble. If base estimators do not implement a ``predict_proba``
    method, then it resorts to voting and the predicted class probabilities
    of an input sample represents the proportion of estimators predicting
    each class.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    Returns
    -------
    p : ndarray of shape (n_samples, n_classes)
        The class probabilities of the input samples. The order of the
        classes corresponds to that in the attribute :term:`classes_`.
    &#34;&#34;&#34;
    all_proba = self.predict_all_proba(X)
    # Reduce
    proba = all_proba.mean(axis=0)

    return proba</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingClassifier.set_fit_request"><code class="name flex">
<span>def <span class="ident">set_fit_request</span></span>(<span>self: crossense.ensemble._bagging.CrossBaggingClassifier, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> crossense.ensemble._bagging.CrossBaggingClassifier</span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>fit</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>fit</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingClassifier.set_score_request"><code class="name flex">
<span>def <span class="ident">set_score_request</span></span>(<span>self: crossense.ensemble._bagging.CrossBaggingClassifier, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> crossense.ensemble._bagging.CrossBaggingClassifier</span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>score</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>score</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>score</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>score</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="crossense.ensemble.CrossBaggingRegressor"><code class="flex name class">
<span>class <span class="ident">CrossBaggingRegressor</span></span>
<span>(</span><span>estimator: object = None, cv: Union[int, BaseCrossValidator, Iterable] = 5, *, n_jobs: Optional[int] = None, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>A cross-validation Bagging regressor.</p>
<p>A Bagging regressor is an ensemble meta-estimator that fits base
regressors each on a fold of cross-validation generator</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>estimator_</code></strong> :&ensp;<code>estimator</code></dt>
<dd>The base estimator from which the ensemble is grown.</dd>
<dt><strong><code>n_features_in_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features seen during :term:<code>fit</code>.</dd>
<dt><strong><code>feature_names_in_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (</code>n_features_in_<code>,)</code></dt>
<dd>Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
has feature names that are all strings.</dd>
<dt><strong><code>estimators_</code></strong> :&ensp;<code>list</code> of <code>estimators</code></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><strong><code>estimators_samples_</code></strong> :&ensp;<code>list</code> of <code>arrays</code></dt>
<dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from sklearn.svm import SVR
&gt;&gt;&gt; from crossense.ensemble import CrossBaggingRegressor
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=4,
...                        n_informative=2, n_targets=1,
...                        random_state=0, shuffle=False)
&gt;&gt;&gt; regr = CrossBaggingRegressor(estimator=SVR(), cv=5).fit(X, y)
&gt;&gt;&gt; regr.predict([[0, 0, 0, 0]])
array([-2.8720...])
</code></pre>
<h2 id="parameters">Parameters</h2>
<p>estimator:
The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a
:class:<code>~sklearn.tree.DecisionTreeClassifier</code>.</p>
<p>cv:
Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<pre><code>- &lt;code&gt;None&lt;/code&gt;, to use the default 5-fold cross validation,
- int, to specify the number of folds in a &lt;code&gt;(Stratified)KFold&lt;/code&gt;,
- :term:&lt;code&gt;CV splitter&lt;/code&gt;,
- An iterable that generates (train, test) splits as arrays of indices.

For &lt;code&gt;int&lt;/code&gt;/&lt;code&gt;None&lt;/code&gt; inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is
either binary or multiclass, :class:&lt;code&gt;StratifiedKFold&lt;/code&gt; is used. In all
other cases, :class:&lt;code&gt;KFold&lt;/code&gt; is used. These splitters are instantiated
with `shuffle=False` so the splits will be the same across calls.

Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
cross-validation strategies that can be used here.
</code></pre>
<p>n_jobs:
The number of jobs to run in parallel for both :meth:<code>fit</code> and
:meth:<code>predict</code>. <code>None</code> means 1 unless in a
:obj:<code>joblib.parallel_backend</code> context. <code>-1</code> means using all
processors. See :term:<code>Glossary &lt;n_jobs&gt;</code> for more details.</p>
<p>verbose:
Controls the verbosity when fitting and predicting.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CrossBaggingRegressor(RegressorMixin, BaseCrossBagging):
    &#34;&#34;&#34;A cross-validation Bagging regressor.

    A Bagging regressor is an ensemble meta-estimator that fits base
    regressors each on a fold of cross-validation generator

    Attributes
    ----------
    estimator_ : estimator
        The base estimator from which the ensemble is grown.

    n_features_in_ : int
        Number of features seen during :term:`fit`.

    feature_names_in_ : ndarray of shape (`n_features_in_`,)
        Names of features seen during :term:`fit`. Defined only when `X`
        has feature names that are all strings.

    estimators_ : list of estimators
        The collection of fitted sub-estimators.

    estimators_samples_ : list of arrays
        The subset of drawn samples (i.e., the in-bag samples) for each base
        estimator. Each subset is defined by an array of the indices selected.

    Examples
    --------
    &gt;&gt;&gt; from sklearn.svm import SVR
    &gt;&gt;&gt; from crossense.ensemble import CrossBaggingRegressor
    &gt;&gt;&gt; from sklearn.datasets import make_regression
    &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=4,
    ...                        n_informative=2, n_targets=1,
    ...                        random_state=0, shuffle=False)
    &gt;&gt;&gt; regr = CrossBaggingRegressor(estimator=SVR(), cv=5).fit(X, y)
    &gt;&gt;&gt; regr.predict([[0, 0, 0, 0]])
    array([-2.8720...])
    &#34;&#34;&#34;

    def __init__(
        self,
        estimator: object = None,
        cv: Union[int, BaseCrossValidator, Iterable] = 5,
        *,
        n_jobs: Optional[int] = None,
        verbose=0,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        estimator:
            The base estimator to fit on random subsets of the dataset.
            If None, then the base estimator is a
            :class:`~sklearn.tree.DecisionTreeClassifier`.

        cv:
            Determines the cross-validation splitting strategy.
            Possible inputs for cv are:

            - `None`, to use the default 5-fold cross validation,
            - int, to specify the number of folds in a `(Stratified)KFold`,
            - :term:`CV splitter`,
            - An iterable that generates (train, test) splits as arrays of indices.

            For `int`/`None` inputs, if the estimator is a classifier and `y` is
            either binary or multiclass, :class:`StratifiedKFold` is used. In all
            other cases, :class:`KFold` is used. These splitters are instantiated
            with `shuffle=False` so the splits will be the same across calls.

            Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
            cross-validation strategies that can be used here.

        n_jobs:
            The number of jobs to run in parallel for both :meth:`fit` and
            :meth:`predict`. ``None`` means 1 unless in a
            :obj:`joblib.parallel_backend` context. ``-1`` means using all
            processors. See :term:`Glossary &lt;n_jobs&gt;` for more details.

        verbose:
            Controls the verbosity when fitting and predicting.
        &#34;&#34;&#34;
        super().__init__(
            estimator=estimator,
            cv=cv,
            n_jobs=n_jobs,
            verbose=verbose,
        )

    def predict_all(self, X):
        &#34;&#34;&#34;Predict regression target of all models for X.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        p : ndarray of shape (n_estimators, n_samples, )
            The predicted values.
        &#34;&#34;&#34;
        # noinspection DuplicatedCode
        check_is_fitted(self)
        # Check data
        X = self._validate_data(
            X,
            accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
            dtype=None,
            force_all_finite=False,
            reset=False,
        )

        # Parallel loop
        n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
            delayed(_parallel_predict_regression)(
                self.estimators_[starts[i] : starts[i + 1]],
                X,
            )
            for i in range(n_jobs)
        )
        all_y_hat = list(itertools.chain.from_iterable(all_y_hat))
        return np.concatenate([x[np.newaxis, :] for x in all_y_hat], axis=0)

    def predict(self, X):
        &#34;&#34;&#34;Predict regression target for X.

        The predicted regression target of an input sample is computed as the
        mean predicted regression targets of the estimators in the ensemble.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        y : ndarray of shape (n_samples,)
            The predicted values.
        &#34;&#34;&#34;
        all_y_hat = self.predict_all(X)
        # Reduce
        y_hat = sum(all_y_hat) / self.n_estimators

        return y_hat

    # noinspection PyMethodOverriding
    def _validate_estimator(self):
        &#34;&#34;&#34;Check the estimator and set the estimator_ attribute.&#34;&#34;&#34;
        super()._validate_estimator(default=DecisionTreeRegressor())

    def _more_tags(self):
        if self.estimator is None:
            estimator = DecisionTreeRegressor()
        else:
            estimator = self.estimator
        return {&#34;allow_nan&#34;: _safe_tags(estimator, &#34;allow_nan&#34;)}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.RegressorMixin</li>
<li>crossense.ensemble._bagging.BaseCrossBagging</li>
<li>sklearn.ensemble._base.BaseEnsemble</li>
<li>sklearn.base.MetaEstimatorMixin</li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="crossense.ensemble.CrossBaggingRegressor.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict regression target for X.</p>
<p>The predicted regression target of an input sample is computed as the
mean predicted regression targets of the estimators in the ensemble.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples,)</code></dt>
<dd>The predicted values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;Predict regression target for X.

    The predicted regression target of an input sample is computed as the
    mean predicted regression targets of the estimators in the ensemble.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    Returns
    -------
    y : ndarray of shape (n_samples,)
        The predicted values.
    &#34;&#34;&#34;
    all_y_hat = self.predict_all(X)
    # Reduce
    y_hat = sum(all_y_hat) / self.n_estimators

    return y_hat</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingRegressor.predict_all"><code class="name flex">
<span>def <span class="ident">predict_all</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict regression target of all models for X.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_estimators, n_samples, )</code></dt>
<dd>The predicted values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_all(self, X):
    &#34;&#34;&#34;Predict regression target of all models for X.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The training input samples. Sparse matrices are accepted only if
        they are supported by the base estimator.

    Returns
    -------
    p : ndarray of shape (n_estimators, n_samples, )
        The predicted values.
    &#34;&#34;&#34;
    # noinspection DuplicatedCode
    check_is_fitted(self)
    # Check data
    X = self._validate_data(
        X,
        accept_sparse=[&#34;csr&#34;, &#34;csc&#34;],
        dtype=None,
        force_all_finite=False,
        reset=False,
    )

    # Parallel loop
    n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)

    all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
        delayed(_parallel_predict_regression)(
            self.estimators_[starts[i] : starts[i + 1]],
            X,
        )
        for i in range(n_jobs)
    )
    all_y_hat = list(itertools.chain.from_iterable(all_y_hat))
    return np.concatenate([x[np.newaxis, :] for x in all_y_hat], axis=0)</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingRegressor.set_fit_request"><code class="name flex">
<span>def <span class="ident">set_fit_request</span></span>(<span>self: crossense.ensemble._bagging.CrossBaggingRegressor, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> crossense.ensemble._bagging.CrossBaggingRegressor</span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>fit</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>fit</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>fit</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>fit</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
<dt id="crossense.ensemble.CrossBaggingRegressor.set_score_request"><code class="name flex">
<span>def <span class="ident">set_score_request</span></span>(<span>self: crossense.ensemble._bagging.CrossBaggingRegressor, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> crossense.ensemble._bagging.CrossBaggingRegressor</span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>score</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>score</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>score</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>score</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(**kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments&#34;
            f&#34; are: {set(self.keys)}&#34;
        )

    requests = instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    instance._metadata_request = requests

    return instance</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="crossense" href="../index.html">crossense</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="crossense.ensemble.tests" href="tests/index.html">crossense.ensemble.tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="crossense.ensemble.BaseCrossBagging" href="#crossense.ensemble.BaseCrossBagging">BaseCrossBagging</a></code></h4>
<ul class="">
<li><code><a title="crossense.ensemble.BaseCrossBagging.fit" href="#crossense.ensemble.BaseCrossBagging.fit">fit</a></code></li>
<li><code><a title="crossense.ensemble.BaseCrossBagging.set_fit_request" href="#crossense.ensemble.BaseCrossBagging.set_fit_request">set_fit_request</a></code></li>
<li><code><a title="crossense.ensemble.BaseCrossBagging.set_params" href="#crossense.ensemble.BaseCrossBagging.set_params">set_params</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="crossense.ensemble.CrossBaggingClassifier" href="#crossense.ensemble.CrossBaggingClassifier">CrossBaggingClassifier</a></code></h4>
<ul class="two-column">
<li><code><a title="crossense.ensemble.CrossBaggingClassifier.decision_function" href="#crossense.ensemble.CrossBaggingClassifier.decision_function">decision_function</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingClassifier.predict" href="#crossense.ensemble.CrossBaggingClassifier.predict">predict</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingClassifier.predict_all_proba" href="#crossense.ensemble.CrossBaggingClassifier.predict_all_proba">predict_all_proba</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingClassifier.predict_log_proba" href="#crossense.ensemble.CrossBaggingClassifier.predict_log_proba">predict_log_proba</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingClassifier.predict_proba" href="#crossense.ensemble.CrossBaggingClassifier.predict_proba">predict_proba</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingClassifier.set_fit_request" href="#crossense.ensemble.CrossBaggingClassifier.set_fit_request">set_fit_request</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingClassifier.set_score_request" href="#crossense.ensemble.CrossBaggingClassifier.set_score_request">set_score_request</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="crossense.ensemble.CrossBaggingRegressor" href="#crossense.ensemble.CrossBaggingRegressor">CrossBaggingRegressor</a></code></h4>
<ul class="">
<li><code><a title="crossense.ensemble.CrossBaggingRegressor.predict" href="#crossense.ensemble.CrossBaggingRegressor.predict">predict</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingRegressor.predict_all" href="#crossense.ensemble.CrossBaggingRegressor.predict_all">predict_all</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingRegressor.set_fit_request" href="#crossense.ensemble.CrossBaggingRegressor.set_fit_request">set_fit_request</a></code></li>
<li><code><a title="crossense.ensemble.CrossBaggingRegressor.set_score_request" href="#crossense.ensemble.CrossBaggingRegressor.set_score_request">set_score_request</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>